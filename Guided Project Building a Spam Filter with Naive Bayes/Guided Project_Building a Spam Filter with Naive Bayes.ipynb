{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we're going to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. **Our goal is to write a program that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).**\n",
    "\n",
    "To train the algorithm, we'll use a dataset of 5,572 SMS messages that are already classified by humans. The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](<https://archive.ics.uci.edu/ml/datasets/sms+spam+collection>). The data collection process is described in more details on [this page](<http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition>), where you can also find some of the papers authored by Tiago A. Almeida and José María Gómez Hidalgo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataset\n",
    "import pandas as pd\n",
    "\n",
    "smsdata = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find how many rows and columns it has\n",
    "print(smsdata.shape)\n",
    "smsdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     87.0\n",
       "spam    13.0\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding what percentage of the messages is spam and what percentage is ham (\"ham\" means non-spam).\n",
    "round(smsdata['Label'].value_counts(normalize=True) * 100, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output reveals that 87% of the messages are ham (\"ham\" means non-spam), and the remaining 13% are spam in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set and Test Set\n",
    "\n",
    "Now that we've become a bit familiar with the dataset, we can move on to building the spam filter.\n",
    "\n",
    "However, before creating it, it's very helpful to first think of a way of testing how well it works. When creating software (a spam filter is software), a good rule of thumb is that designing the test comes before creating the software. If we write the software first, then it's tempting to come up with a biased test just to make sure the software passes it.\n",
    "\n",
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "* A training set, which we'll use to \"train\" the computer how to classify messages.\n",
    "* A test set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "* The training set will have 4,458 messages (about 80% of the dataset).\n",
    "* The test set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "To better understand the purpose of putting a test set aside, let's begin by observing that all 1,114 messages in our test set are already classified by a human. When the spam filter is ready, we're going to treat these messages as new and have the filter classify them. Once we have the results, we'll be able to compare the algorithm classification with that done by a human initially, and this way we'll see how good the spam filter really is.\n",
    "\n",
    "As mentioned before, our goal is to create a spam filter that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam) with our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to start by randomizing the entire dataset to ensure that spam and ham messages are spread properly throughout\n",
    "#the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the frac=1 parameter to randomize the entire dataset.\n",
    "#Using the random_state=1 parameter to make sure the results are reproducible.\n",
    "smsdatapreparation = smsdata.sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the above randomized dataset into a training and a test set. The training set should account for 80% of the dataset,\n",
    "#as mentioned earlier and the remaining 20% of the data should be the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_index = round(len(smsdatapreparation) * 0.8)\n",
    "\n",
    "training_set = smsdatapreparation[:training_set_index].reset_index(drop=True)\n",
    "test_set = smsdatapreparation[training_set_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that the training set have 4,458 messages (about 80% of the dataset) and \n",
    "#the test set have 1,114 messages (about 20% of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 2)\n",
      "(1114, 2)\n"
     ]
    }
   ],
   "source": [
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the percentage of spam and ham in both the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     87.0\n",
       "spam    13.0\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(training_set['Label'].value_counts(normalize=True) * 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above output shows our training set is a perfect representative of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     87.0\n",
       "spam    13.0\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(test_set['Label'].value_counts(normalize=True) * 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all seems good to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the training set to teach the algorithm to classify new messages.\n",
    "\n",
    "When a new message comes in, our Naive Bayes algorithm will make the classification based on the results it gets through these two equations:\n",
    "\n",
    "`P(Spam|w1,w2,...,wn) ∝ P(Spam) ⋅ ∏i=1nP(wi|Spam)`\n",
    "\n",
    "`P(Ham|w1,w2,...,wn) ∝ P(Ham) ⋅ ∏i=1nP(wi|Ham)`\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we need to use these equations:\n",
    "\n",
    "`P(wi|Spam)=Nwi|Spam + α / NSpam + α ⋅ NVocabulary`\n",
    "\n",
    "`P(wi|Ham)=Nwi|Ham + α / NHam + α ⋅ NVocabulary`\n",
    "\n",
    "Summarizing what the terms in the equations above mean:\n",
    "\n",
    "* Nwi|Spam=the number of times the word wi occurs in Spam messages\n",
    "* Nwi|Ham=the number of times the word wi occurs in Ham messages\n",
    "* NSpam=total number of words in spam messages\n",
    "* NHam=total number of words in non-spam messages\n",
    "* NVocabulary=total number of words in the vocabulary\n",
    "* α=1    (α is a smoothing parameter)\n",
    "\n",
    "To calculate all these probabilities, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need. \n",
    "\n",
    "Right now, our training and test sets have this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'SMS'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'SMS'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham          Later i guess. I needa do mcat study too.\n",
       "1   ham             But i haf enuff space got like 4 mb...\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...\n",
       "4   ham  All done, all handed in. Don't know if mega sh..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the calculations easier, we want bring the data to the format by cleaning and transforming:\n",
    "\n",
    "* The SMS column will not exist anymore, instead the SMS column will be replaced by a series of new columns, where each column represents a unique word from the vocabulary.\n",
    "* Each row will describe a single message and in the vocabulary word columns it will have the values representing the number of times these words are present in that message (frequency).\n",
    "* All words in the vocabulary are in lower case for uniformity, so the message in our dataset should too be in lower case.\n",
    "* Punctuation will not be taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data cleaning process by removing the punctuation and bringing all the words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all the punctuation from the SMS column by using the regex '\\W' \n",
    "#to detect any character that is not from a-z, A-Z or 0-9.\n",
    "#For instance, the function re.sub('\\W', ' ', 'Secret!! Money, goods.' ) strips all the punctuation marks and \n",
    "#outputs the string as 'Secret Money goods '.\n",
    "#For simplicity, we will be using the Series.str.replace() method here instead for removing all the punctuation and \n",
    "#Series.str.lower() method for transforming every letter in every word to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.replace('\\W', ' ')\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Vocabulary  -  a set of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a vocabulary for the messages in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming each message from the SMS column into a list by splitting the string at the space character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Python list containing all the unique words across all messages, where each word is represented as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['armenia',\n",
       " 'gbp',\n",
       " 'prem',\n",
       " 'tscs',\n",
       " 'fated',\n",
       " 'hp',\n",
       " 'maximum',\n",
       " '27',\n",
       " 'sugababes',\n",
       " 'misss',\n",
       " 'best',\n",
       " 'quizclub',\n",
       " 'bmw',\n",
       " 'places',\n",
       " 'picsfree1',\n",
       " 'worlds',\n",
       " 'snowman',\n",
       " 'tsandcs',\n",
       " 'w1t1jy',\n",
       " 'nat',\n",
       " '08719181259',\n",
       " 'lotto',\n",
       " 'devils',\n",
       " 'ntt',\n",
       " '50award',\n",
       " 'heal',\n",
       " 'lookin',\n",
       " 'gdeve',\n",
       " 'transfr',\n",
       " 'packing',\n",
       " 'ymca',\n",
       " 'sisters',\n",
       " 'hills',\n",
       " 'than',\n",
       " 'lager',\n",
       " 'happily',\n",
       " '08718726970',\n",
       " 'scenario',\n",
       " 'nike',\n",
       " '087123002209am',\n",
       " 'yoyyooo',\n",
       " 'vpod',\n",
       " 'openin',\n",
       " 'muht',\n",
       " 'das',\n",
       " 'exams',\n",
       " 'type',\n",
       " 'compulsory',\n",
       " 'nokia6650',\n",
       " 'colleagues',\n",
       " 'quiet',\n",
       " 'la',\n",
       " 'commercial',\n",
       " 'invnted',\n",
       " 'deliveredtomorrow',\n",
       " 'pshew',\n",
       " 'anniversary',\n",
       " 'syria',\n",
       " 'mtmsg',\n",
       " 'haul',\n",
       " 'songs',\n",
       " 'cheered',\n",
       " 'skyped',\n",
       " 'choose',\n",
       " 'polyc',\n",
       " 'goodmate',\n",
       " 'coat',\n",
       " 'weather',\n",
       " 'll',\n",
       " 'resubbing',\n",
       " 'wasted',\n",
       " 'fingers',\n",
       " 'slip',\n",
       " 'glorious',\n",
       " '087187262701',\n",
       " 'goodnoon',\n",
       " 'indian',\n",
       " 'webeburnin',\n",
       " 'nange',\n",
       " 'zaher',\n",
       " 'cousin',\n",
       " 'solve',\n",
       " 'thats',\n",
       " 'sub',\n",
       " 'nursery',\n",
       " '09058091870',\n",
       " '1250',\n",
       " 'bawling',\n",
       " 'php',\n",
       " 'display',\n",
       " 'arguing',\n",
       " 'market',\n",
       " 'till',\n",
       " 'std',\n",
       " 'shrub',\n",
       " 'varunnathu',\n",
       " 'meh',\n",
       " 'showr',\n",
       " '24th',\n",
       " 'wondering',\n",
       " 'user',\n",
       " 'shore',\n",
       " 'cried',\n",
       " 'arun',\n",
       " 'american',\n",
       " 'guaranteed',\n",
       " 'mis',\n",
       " 'monos',\n",
       " 'walking',\n",
       " 'triumphed',\n",
       " 'past',\n",
       " 'earliest',\n",
       " 'ktv',\n",
       " 'refunded',\n",
       " 'goodnight',\n",
       " 'reaching',\n",
       " 'xxxxx',\n",
       " 'present',\n",
       " 'brain',\n",
       " 'trusting',\n",
       " 'nobbing',\n",
       " 'dial',\n",
       " 'unsub',\n",
       " 'jsco',\n",
       " 'creep',\n",
       " 'recorder',\n",
       " 'wat',\n",
       " 'urgran',\n",
       " 'mistake',\n",
       " 'speaking',\n",
       " 'waht',\n",
       " 'audrie',\n",
       " 'simulate',\n",
       " 'mjzgroup',\n",
       " 'sathy',\n",
       " 'afew',\n",
       " 'schedule',\n",
       " 'indians',\n",
       " 'tick',\n",
       " 'depression',\n",
       " 'stamps',\n",
       " 'kickboxing',\n",
       " 'jobs',\n",
       " '50gbp',\n",
       " 'ruining',\n",
       " 'handsome',\n",
       " 'm95',\n",
       " 'mobile',\n",
       " 'dhina',\n",
       " 'paragon',\n",
       " 'reasons',\n",
       " 'brought',\n",
       " 'evry',\n",
       " '20p',\n",
       " 'wasnt',\n",
       " 'canceled',\n",
       " '0871277810810',\n",
       " 'wont',\n",
       " 'taxless',\n",
       " 'aid',\n",
       " 'garage',\n",
       " 'carefully',\n",
       " 'pull',\n",
       " 'auction',\n",
       " 'helen',\n",
       " 'ummifying',\n",
       " 't91',\n",
       " '87077',\n",
       " 'liao',\n",
       " 'brighten',\n",
       " 'soil',\n",
       " 'fifth',\n",
       " 'contract',\n",
       " 'recently',\n",
       " 'ended',\n",
       " 'elephant',\n",
       " 'wesleys',\n",
       " 'july',\n",
       " '09096102316',\n",
       " 'sack',\n",
       " 'crying',\n",
       " 'smith',\n",
       " 'fones',\n",
       " 'hot',\n",
       " 'secretary',\n",
       " 'jas',\n",
       " 'basically',\n",
       " '3rd',\n",
       " 'attributed',\n",
       " 'm26',\n",
       " 'complete',\n",
       " 'reache',\n",
       " 'colourful',\n",
       " 'splash',\n",
       " 'slovely',\n",
       " '0871212025016',\n",
       " 'maps',\n",
       " 'genus',\n",
       " 'centre',\n",
       " 'eurodisinc',\n",
       " 'logoff',\n",
       " 'neft',\n",
       " 'membership',\n",
       " 'trauma',\n",
       " 'pints',\n",
       " '200',\n",
       " 'nattil',\n",
       " 'hppnss',\n",
       " 'surly',\n",
       " 'plus',\n",
       " 'warning',\n",
       " 'permanent',\n",
       " 'chechi',\n",
       " 'products',\n",
       " 'unknown',\n",
       " 'burgundy',\n",
       " 'needed',\n",
       " 'award',\n",
       " 'test',\n",
       " 'subscription',\n",
       " 'kodthini',\n",
       " '09058095201',\n",
       " 'steed',\n",
       " 'jap',\n",
       " 'sweets',\n",
       " 'politicians',\n",
       " 'mountain',\n",
       " 'pix',\n",
       " 'henry',\n",
       " 'saw',\n",
       " 'cardiff',\n",
       " 'place',\n",
       " 'hv',\n",
       " 'vitamin',\n",
       " '4fil',\n",
       " 'sux',\n",
       " 'com1win150ppmx3age16subscription',\n",
       " 'troubleshooting',\n",
       " 'advance',\n",
       " 'goverment',\n",
       " 'hunt',\n",
       " '2667',\n",
       " 'unusual',\n",
       " 'byatch',\n",
       " 'take',\n",
       " 'lionm',\n",
       " 'frying',\n",
       " 'funs',\n",
       " 'irritating',\n",
       " 'throat',\n",
       " 'jump',\n",
       " '38',\n",
       " 'stock',\n",
       " 'normally',\n",
       " '30th',\n",
       " 'canada',\n",
       " 'anythingtomorrow',\n",
       " 'sara',\n",
       " 'datz',\n",
       " 'dismissial',\n",
       " 'uttered',\n",
       " 'snow',\n",
       " 'teletext',\n",
       " 'sandiago',\n",
       " 'wizzle',\n",
       " 'polyh',\n",
       " 'rudi',\n",
       " 'reflection',\n",
       " 'farm',\n",
       " 'even',\n",
       " 'newscaster',\n",
       " 'kerala',\n",
       " 'hogolo',\n",
       " 'amrita',\n",
       " 'harri',\n",
       " '83021',\n",
       " 'sometimes',\n",
       " '545',\n",
       " 'bedrm',\n",
       " 'thia',\n",
       " 'pobox75ldns7',\n",
       " 'eatin',\n",
       " 'swiss',\n",
       " 'lekdog',\n",
       " 'sounding',\n",
       " 'hides',\n",
       " '69911',\n",
       " 'victors',\n",
       " 'ji',\n",
       " 'facebook',\n",
       " 'hanging',\n",
       " 'accumulation',\n",
       " 'wicklow',\n",
       " 'explicit',\n",
       " 'thinked',\n",
       " 'gee',\n",
       " 'laughs',\n",
       " 'argentina',\n",
       " 'come',\n",
       " 'lab',\n",
       " 'presnts',\n",
       " 'nevamind',\n",
       " '09064015307',\n",
       " 'delivered',\n",
       " 'pre',\n",
       " '09065171142',\n",
       " 'peaceful',\n",
       " 'soooo',\n",
       " 'bulbs',\n",
       " 'step',\n",
       " '08712317606',\n",
       " 'now1',\n",
       " 'archive',\n",
       " '0721072',\n",
       " 'sayy',\n",
       " 'formally',\n",
       " 'cruise',\n",
       " '0quit',\n",
       " 'pan',\n",
       " 'showrooms',\n",
       " '2nhite',\n",
       " 'clocks',\n",
       " 'ystrday',\n",
       " 'offer',\n",
       " 'players',\n",
       " 'fix',\n",
       " '1000s',\n",
       " 'idc',\n",
       " 'andre',\n",
       " 'teresa',\n",
       " 'have',\n",
       " 'dogging',\n",
       " 'ws',\n",
       " 'morphine',\n",
       " 'ultimatum',\n",
       " 'bill',\n",
       " 'backdoor',\n",
       " 'tips',\n",
       " '87121',\n",
       " 'imposter',\n",
       " 'bedbut',\n",
       " 'omw',\n",
       " 'badass',\n",
       " 'teachers',\n",
       " 'hol',\n",
       " 'movies',\n",
       " '69988',\n",
       " 'dat',\n",
       " 'mmm',\n",
       " 'bandages',\n",
       " 'deposited',\n",
       " 'wk',\n",
       " 'sen',\n",
       " 'those',\n",
       " 'sullivan',\n",
       " 'handing',\n",
       " 'sections',\n",
       " 'loverboy',\n",
       " 'sambar',\n",
       " 'follow',\n",
       " 'profile',\n",
       " '09071512432',\n",
       " 'chatter',\n",
       " '08715705022',\n",
       " 'infact',\n",
       " 'imat',\n",
       " 'gotany',\n",
       " 'plan',\n",
       " 'yi',\n",
       " 'rupaul',\n",
       " 'him',\n",
       " 'photos',\n",
       " 'shu',\n",
       " 'gotta',\n",
       " 'friendships',\n",
       " 'hotel',\n",
       " 'bluray',\n",
       " 'dontcha',\n",
       " 'camp',\n",
       " 'two',\n",
       " 'knew',\n",
       " 'fancied',\n",
       " 'gayle',\n",
       " 'tactless',\n",
       " 'oath',\n",
       " 'flight',\n",
       " '9280114',\n",
       " 'weakness',\n",
       " 'ups',\n",
       " 'treacle',\n",
       " 'science',\n",
       " 'has',\n",
       " 'cheque',\n",
       " 'replying',\n",
       " 'charged',\n",
       " 'wright',\n",
       " 'dabooks',\n",
       " 'aroundn',\n",
       " 'baig',\n",
       " 'female',\n",
       " 'barrel',\n",
       " 'cloud',\n",
       " 'lick',\n",
       " 'fake',\n",
       " 'braved',\n",
       " 'grave',\n",
       " 'cnupdates',\n",
       " 'dieting',\n",
       " 'pile',\n",
       " 'tootsie',\n",
       " 'maruti',\n",
       " 'ful',\n",
       " 'style',\n",
       " 'coveragd',\n",
       " '08717898035',\n",
       " 'hvae',\n",
       " 'itna',\n",
       " 'could',\n",
       " 'apples',\n",
       " 'electricity',\n",
       " 'tasts',\n",
       " 'max10mins',\n",
       " 'mmsto',\n",
       " 'bar',\n",
       " 'astronomer',\n",
       " 'vettam',\n",
       " '7zs',\n",
       " 'interested',\n",
       " 'roommate',\n",
       " '08712460324',\n",
       " 'zyada',\n",
       " 'kilos',\n",
       " 'cya',\n",
       " 'hectic',\n",
       " 'rush',\n",
       " 'balloon',\n",
       " 'tongued',\n",
       " 'yourjob',\n",
       " 'worth',\n",
       " 'jokes',\n",
       " 'weeks',\n",
       " 'papa',\n",
       " 'behind',\n",
       " 'claimcode',\n",
       " 'exact',\n",
       " 'kindly',\n",
       " 'rentl',\n",
       " 'c',\n",
       " 'othrwise',\n",
       " '2morow',\n",
       " 'tncs',\n",
       " 'enufcredeit',\n",
       " 'tonights',\n",
       " 'broadband',\n",
       " 'muah',\n",
       " '9t',\n",
       " '4',\n",
       " 'finally',\n",
       " 'wikipedia',\n",
       " 'addamsfa',\n",
       " 'stone',\n",
       " '50ea',\n",
       " 'delete',\n",
       " 'meal',\n",
       " '113',\n",
       " 'earn',\n",
       " 'crucial',\n",
       " 'swatch',\n",
       " 'necklace',\n",
       " 'gre',\n",
       " 'uve',\n",
       " 'parked',\n",
       " '8007',\n",
       " 'club4mobiles',\n",
       " 'natural',\n",
       " '08712402050',\n",
       " '84122',\n",
       " 'worthless',\n",
       " 'deposit',\n",
       " 'springs',\n",
       " 'ing',\n",
       " 'yifeng',\n",
       " 'previous',\n",
       " 'respond',\n",
       " 'matched',\n",
       " 'baller',\n",
       " 'yards',\n",
       " 'myself',\n",
       " 'silver',\n",
       " 'proze',\n",
       " 'deeraj',\n",
       " 'legal',\n",
       " 'sez',\n",
       " 'department',\n",
       " 'cinema',\n",
       " 'prakasam',\n",
       " 'hi',\n",
       " 'far',\n",
       " 'cross',\n",
       " 'callback',\n",
       " 'cuppa',\n",
       " 'light',\n",
       " 'lou',\n",
       " 'hanuman',\n",
       " 'expensive',\n",
       " 'sw73ss',\n",
       " 'truly',\n",
       " 'jersey',\n",
       " 'al',\n",
       " 'wc1n3xx',\n",
       " 'ileave',\n",
       " 'cake',\n",
       " 'restocked',\n",
       " 'gosh',\n",
       " 'xin',\n",
       " 'people',\n",
       " 'complacent',\n",
       " 'o2',\n",
       " 'fly',\n",
       " 'gain',\n",
       " 'related',\n",
       " 'cup',\n",
       " 'nalli',\n",
       " 'connect',\n",
       " 'oyea',\n",
       " 'watching',\n",
       " 'signin',\n",
       " 'wylie',\n",
       " '7250i',\n",
       " 'companion',\n",
       " 'dane',\n",
       " 'comfort',\n",
       " 'anythin',\n",
       " 'redeemed',\n",
       " 'team',\n",
       " 'shower',\n",
       " 'shorter',\n",
       " 'nigpun',\n",
       " 'justthought',\n",
       " 'whats',\n",
       " 'glands',\n",
       " 'twins',\n",
       " 'losing',\n",
       " '220cm2',\n",
       " 'moby',\n",
       " 'cabin',\n",
       " 'ericson',\n",
       " 'tells',\n",
       " 'october',\n",
       " 'prayers',\n",
       " 'glasgow',\n",
       " 'atleast',\n",
       " 'corrct',\n",
       " 'toaday',\n",
       " 'syd',\n",
       " 'pavanaputra',\n",
       " 'do',\n",
       " 'z',\n",
       " 'swtheart',\n",
       " 'nyc',\n",
       " 'magazine',\n",
       " 'toppoly',\n",
       " 'm8s',\n",
       " '2stoptxt',\n",
       " '08712402902',\n",
       " 'ryan',\n",
       " 'icon',\n",
       " 'chikku',\n",
       " 'csc',\n",
       " 'mirror',\n",
       " '8883',\n",
       " 'beta',\n",
       " 'map',\n",
       " '06',\n",
       " 'gona',\n",
       " 'afghanistan',\n",
       " 'ritten',\n",
       " 'timin',\n",
       " 'enjoyin',\n",
       " 'uniform',\n",
       " 'sleeping',\n",
       " 'classmates',\n",
       " 'posts',\n",
       " 'phb1',\n",
       " 'previews',\n",
       " 'unsold',\n",
       " 'woohoo',\n",
       " 'slurp',\n",
       " 'rwm',\n",
       " '09064018838',\n",
       " 'fudge',\n",
       " '18',\n",
       " 'cds',\n",
       " 'say',\n",
       " '40mph',\n",
       " '09061743386',\n",
       " 'mid',\n",
       " 'dnot',\n",
       " 'prior',\n",
       " 'nit',\n",
       " 'evn',\n",
       " 'jd',\n",
       " 'planet',\n",
       " '07123456789',\n",
       " 'pocay',\n",
       " 'super',\n",
       " 'voila',\n",
       " 'felt',\n",
       " 'latests',\n",
       " 'practicing',\n",
       " 'neck',\n",
       " 'jod',\n",
       " 'ringtones',\n",
       " 'flower',\n",
       " '08718711108',\n",
       " 'starving',\n",
       " 'tahan',\n",
       " 'studio',\n",
       " 'messaging',\n",
       " 'blessed',\n",
       " 'lyricalladie',\n",
       " 'specific',\n",
       " 'priority',\n",
       " 'computerless',\n",
       " 'settling',\n",
       " 'serving',\n",
       " 'sarasota',\n",
       " 'real',\n",
       " 'spinout',\n",
       " '2morrow',\n",
       " 'ffectionate',\n",
       " 'aproach',\n",
       " '872',\n",
       " 'rs',\n",
       " 'act',\n",
       " 'login',\n",
       " '087018728737',\n",
       " 'leaves',\n",
       " 'arng',\n",
       " 'enjoyed',\n",
       " 'jones',\n",
       " 'sliding',\n",
       " '09056242159',\n",
       " 'notice',\n",
       " 'waiting',\n",
       " '09041940223',\n",
       " '2optout',\n",
       " 'seing',\n",
       " 'achieve',\n",
       " 'desperate',\n",
       " 'edwards',\n",
       " 'bubbletext',\n",
       " 'ctxt',\n",
       " 'arab',\n",
       " 'ink',\n",
       " '09065989182',\n",
       " 'way',\n",
       " 'fgkslpo',\n",
       " 'stayed',\n",
       " 'network',\n",
       " '08081263000',\n",
       " 'kappa',\n",
       " '0870737910216yrs',\n",
       " 'customers',\n",
       " 'finding',\n",
       " 'salad',\n",
       " 'inspection',\n",
       " 'clubzed',\n",
       " 'havin',\n",
       " 'poker',\n",
       " '69866',\n",
       " 'per',\n",
       " '54',\n",
       " 'wisdom',\n",
       " 'his',\n",
       " 'punch',\n",
       " 'axis',\n",
       " 'tmrw',\n",
       " 'mobypobox734ls27yf',\n",
       " 'btw',\n",
       " 'trip',\n",
       " 'between',\n",
       " 'pause',\n",
       " 'rolled',\n",
       " 'challenge',\n",
       " 'together',\n",
       " 'massive',\n",
       " 'northampton',\n",
       " '80488',\n",
       " 'gud',\n",
       " 'notifications',\n",
       " 'hg',\n",
       " 'praps',\n",
       " 'anything',\n",
       " 'god',\n",
       " 'much',\n",
       " 'blonde',\n",
       " 'pobox84',\n",
       " 'hey',\n",
       " 'grins',\n",
       " '2wks',\n",
       " 'station',\n",
       " 'checked',\n",
       " 'unsubscribed',\n",
       " 'hasn',\n",
       " 'shade',\n",
       " 'unhappy',\n",
       " 'b4',\n",
       " 'wkent',\n",
       " 'feathery',\n",
       " 'anderson',\n",
       " 'mentionned',\n",
       " 'probthat',\n",
       " 'tyler',\n",
       " 'situations',\n",
       " 'bring',\n",
       " 'happiness',\n",
       " 'problem',\n",
       " 'datebox1282essexcm61xn',\n",
       " 'oja',\n",
       " '07821230901',\n",
       " 'jjc',\n",
       " '09099726481',\n",
       " 'amused',\n",
       " 'citizen',\n",
       " 'spain',\n",
       " 'trade',\n",
       " 'dined',\n",
       " 'youwanna',\n",
       " 'pap',\n",
       " 'tel',\n",
       " 'bday',\n",
       " 'ugo',\n",
       " 'timing',\n",
       " 'accomodate',\n",
       " 'copied',\n",
       " 'costumes',\n",
       " '2005',\n",
       " 'secured',\n",
       " 'manual',\n",
       " 'woah',\n",
       " 'maintaining',\n",
       " 'better',\n",
       " 'fiend',\n",
       " 'asked',\n",
       " 'shipping',\n",
       " 'musta',\n",
       " 'tones2you',\n",
       " 'raj',\n",
       " 'rum',\n",
       " 'bari',\n",
       " 'bakrid',\n",
       " 'vomit',\n",
       " 'wannatell',\n",
       " 'rebtel',\n",
       " 'reality',\n",
       " 'public',\n",
       " 'won',\n",
       " 'danger',\n",
       " 'totes',\n",
       " 'goodevening',\n",
       " 'debating',\n",
       " 'zealand',\n",
       " 'persolvo',\n",
       " 'addicted',\n",
       " 'fear',\n",
       " 'bruv',\n",
       " 'bell',\n",
       " 'sink',\n",
       " 'cash',\n",
       " 'heroes',\n",
       " 'opinions',\n",
       " 'thursday',\n",
       " '0870141701216',\n",
       " 'cave',\n",
       " 'their',\n",
       " 'inr',\n",
       " 'sol',\n",
       " 'hai',\n",
       " 'duchess',\n",
       " 'conditions',\n",
       " 'hour',\n",
       " 'lost',\n",
       " '0844',\n",
       " 'dime',\n",
       " '945',\n",
       " 'blah',\n",
       " 'programs',\n",
       " 'key',\n",
       " 'gist',\n",
       " 'ams',\n",
       " 'epsilon',\n",
       " 'unfortuntly',\n",
       " 'inc',\n",
       " '〨ud',\n",
       " 'shopping',\n",
       " 'medical',\n",
       " 'regarding',\n",
       " 'langport',\n",
       " 'fantastic',\n",
       " 'pobox12n146tf15',\n",
       " 'polyphonic',\n",
       " 'cold',\n",
       " 'age23',\n",
       " 'hamper',\n",
       " 'fancy',\n",
       " 'murderer',\n",
       " 'shelf',\n",
       " 'went',\n",
       " 'virgin',\n",
       " 'footbl',\n",
       " 'soon',\n",
       " 'feelingood',\n",
       " 'football',\n",
       " 'hon',\n",
       " 'use',\n",
       " 'recycling',\n",
       " 'lecture',\n",
       " 'colany',\n",
       " 'steam',\n",
       " 'lasting',\n",
       " 'kanagu',\n",
       " 'pdate_now',\n",
       " 'south',\n",
       " 'bird',\n",
       " 'find',\n",
       " 'fancies',\n",
       " 'pls',\n",
       " 'merememberin',\n",
       " 'wa14',\n",
       " 'diddy',\n",
       " 'venaam',\n",
       " 'campus',\n",
       " 'themob',\n",
       " 'abiola',\n",
       " 'howard',\n",
       " 'orange',\n",
       " '09061701939',\n",
       " 'ummmmmaah',\n",
       " 'vaazhthukkal',\n",
       " 'destination',\n",
       " 'payee',\n",
       " 'uncountable',\n",
       " 'korli',\n",
       " 'philosophical',\n",
       " 'features',\n",
       " 'hopefully',\n",
       " 'with',\n",
       " '9am',\n",
       " '08712400602450p',\n",
       " 'thx',\n",
       " 'sportsx',\n",
       " 'blanked',\n",
       " 'engagement',\n",
       " 'ldnw15h',\n",
       " 'company',\n",
       " 'toyota',\n",
       " 'xxxxxxx',\n",
       " 'avin',\n",
       " 'prayrs',\n",
       " 'ending',\n",
       " 'wildlife',\n",
       " 'xxx',\n",
       " 'wherre',\n",
       " 'authorise',\n",
       " 'failure',\n",
       " 'village',\n",
       " 'flash',\n",
       " 'nxt',\n",
       " 'arnt',\n",
       " '5226',\n",
       " 'sang',\n",
       " 'anand',\n",
       " 'credit',\n",
       " '08000407165',\n",
       " 'blankets',\n",
       " 'guidance',\n",
       " 'yunny',\n",
       " 'ecstacy',\n",
       " 'plz',\n",
       " '3optical',\n",
       " 'echo',\n",
       " 'girlfrnd',\n",
       " 'attractive',\n",
       " 'increments',\n",
       " 'unspoken',\n",
       " 'kitty',\n",
       " 'likes',\n",
       " 'fans',\n",
       " 'consensus',\n",
       " 'exhausted',\n",
       " 'escalator',\n",
       " 'drivin',\n",
       " 'subscriptions',\n",
       " 'addie',\n",
       " 'tacos',\n",
       " 'cysts',\n",
       " 'hex',\n",
       " 'the4th',\n",
       " 'lubly',\n",
       " 'reasonable',\n",
       " 'beware',\n",
       " 'mark',\n",
       " '08712103738',\n",
       " 'dey',\n",
       " 'order',\n",
       " 'closes',\n",
       " 'frens',\n",
       " 'laready',\n",
       " 'frosty',\n",
       " 'alaipayuthe',\n",
       " 'chic',\n",
       " 'geting',\n",
       " 'hugging',\n",
       " 'hostile',\n",
       " 'dhoni',\n",
       " 'outbid',\n",
       " 'vinobanagar',\n",
       " 'general',\n",
       " 'seeking',\n",
       " 'macha',\n",
       " 'virtual',\n",
       " 'recount',\n",
       " 'status',\n",
       " 'sorrow',\n",
       " 'doug',\n",
       " 'coherently',\n",
       " 'humans',\n",
       " 'lesson',\n",
       " 'statement',\n",
       " 'lark',\n",
       " '07815296484',\n",
       " 'amount',\n",
       " 'gaytextbuddy',\n",
       " 'expected',\n",
       " '730',\n",
       " 'actor',\n",
       " '5000',\n",
       " 'bcums',\n",
       " 'notified',\n",
       " 'paper',\n",
       " 'aka',\n",
       " 'invaders',\n",
       " 'halla',\n",
       " 'simpsons',\n",
       " 'qing',\n",
       " 'thanku',\n",
       " '9153',\n",
       " 'avent',\n",
       " 'hundred',\n",
       " 'supply',\n",
       " 'increase',\n",
       " 'lodge',\n",
       " 'rcd',\n",
       " 'ran',\n",
       " 'instantly',\n",
       " '82324',\n",
       " 'nights',\n",
       " 'workage',\n",
       " '2nd',\n",
       " 'shame',\n",
       " 'hunting',\n",
       " 'intentions',\n",
       " 'sitll',\n",
       " 'software',\n",
       " 'chk',\n",
       " 'ovulation',\n",
       " 'cheyyamo',\n",
       " 'constant',\n",
       " 'called',\n",
       " 'again',\n",
       " 'sheffield',\n",
       " 'yoga',\n",
       " 'reward',\n",
       " 'poboxox36504w45wq',\n",
       " 'wine',\n",
       " 'prof',\n",
       " 'korean',\n",
       " 'pooja',\n",
       " 'nino',\n",
       " 'spiffing',\n",
       " 'prsn',\n",
       " '50',\n",
       " 'effects',\n",
       " 'honeybee',\n",
       " 'hit',\n",
       " 'manage',\n",
       " 'spatula',\n",
       " 'mrt',\n",
       " '09064017295',\n",
       " 'nigh',\n",
       " 'fool',\n",
       " 'letters',\n",
       " 'trackmarque',\n",
       " 'street',\n",
       " 'natalie',\n",
       " 'outside',\n",
       " 'sch',\n",
       " 'nusstu',\n",
       " 'fiting',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for message in training_set['SMS']:\n",
    "    for word in message:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "vocabulary = list(set(vocabulary))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total numbers of unique words in vocabulary\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the vocabulary to make the data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>armenia</th>\n",
       "      <th>gbp</th>\n",
       "      <th>prem</th>\n",
       "      <th>tscs</th>\n",
       "      <th>fated</th>\n",
       "      <th>hp</th>\n",
       "      <th>maximum</th>\n",
       "      <th>27</th>\n",
       "      <th>sugababes</th>\n",
       "      <th>misss</th>\n",
       "      <th>...</th>\n",
       "      <th>released</th>\n",
       "      <th>away</th>\n",
       "      <th>surprise</th>\n",
       "      <th>08714712388</th>\n",
       "      <th>bx526</th>\n",
       "      <th>85023</th>\n",
       "      <th>hav2hear</th>\n",
       "      <th>scraped</th>\n",
       "      <th>165</th>\n",
       "      <th>pleasant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   armenia  gbp  prem  tscs  fated  hp  maximum  27  sugababes  misss  ...  \\\n",
       "0        0    0     0     0      0   0        0   0          0      0  ...   \n",
       "1        0    0     0     0      0   0        0   0          0      0  ...   \n",
       "2        0    0     0     0      0   0        0   0          0      0  ...   \n",
       "3        0    0     0     0      0   0        0   0          0      0  ...   \n",
       "4        0    0     0     0      0   0        0   0          0      0  ...   \n",
       "\n",
       "   released  away  surprise  08714712388  bx526  85023  hav2hear  scraped  \\\n",
       "0         0     0         0            0      0      0         0        0   \n",
       "1         0     0         0            0      0      0         0        0   \n",
       "2         0     0         0            0      0      0         0        0   \n",
       "3         0     0         0            0      0      0         0        0   \n",
       "4         0     0         0            0      0      0         0        0   \n",
       "\n",
       "   165  pleasant  \n",
       "0    0         0  \n",
       "1    0         0  \n",
       "2    0         0  \n",
       "3    0         0  \n",
       "4    0         0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabularydata = pd.DataFrame(word_counts_per_sms)\n",
    "vocabularydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the DataFrame we just built above with the DataFrame containing the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>armenia</th>\n",
       "      <th>gbp</th>\n",
       "      <th>prem</th>\n",
       "      <th>tscs</th>\n",
       "      <th>fated</th>\n",
       "      <th>hp</th>\n",
       "      <th>maximum</th>\n",
       "      <th>27</th>\n",
       "      <th>...</th>\n",
       "      <th>released</th>\n",
       "      <th>away</th>\n",
       "      <th>surprise</th>\n",
       "      <th>08714712388</th>\n",
       "      <th>bx526</th>\n",
       "      <th>85023</th>\n",
       "      <th>hav2hear</th>\n",
       "      <th>scraped</th>\n",
       "      <th>165</th>\n",
       "      <th>pleasant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  armenia  gbp  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]        0    0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...        0    0   \n",
       "2   ham                    [welp, apparently, he, retired]        0    0   \n",
       "3   ham                                           [havent]        0    0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...        0    0   \n",
       "\n",
       "   prem  tscs  fated  hp  maximum  27  ...  released  away  surprise  \\\n",
       "0     0     0      0   0        0   0  ...         0     0         0   \n",
       "1     0     0      0   0        0   0  ...         0     0         0   \n",
       "2     0     0      0   0        0   0  ...         0     0         0   \n",
       "3     0     0      0   0        0   0  ...         0     0         0   \n",
       "4     0     0      0   0        0   0  ...         0     0         0   \n",
       "\n",
       "   08714712388  bx526  85023  hav2hear  scraped  165  pleasant  \n",
       "0            0      0      0         0        0    0         0  \n",
       "1            0      0      0         0        0    0         0  \n",
       "2            0      0      0         0        0    0         0  \n",
       "3            0      0      0         0        0    0         0  \n",
       "4            0      0      0         0        0    0         0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_training_set = pd.concat([training_set, vocabularydata], axis=1)\n",
    "final_training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the spam filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had discuss 4 equations above to built our spam filter, let us just revise once more:\n",
    "\n",
    "`P(Spam|w1,w2,...,wn) ∝ P(Spam) ⋅ ∏i=1nP(wi|Spam)`\n",
    "\n",
    "`P(Ham|w1,w2,...,wn) ∝ P(Ham) ⋅ ∏i=1nP(wi|Ham)`\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we need to use these equations:\n",
    "\n",
    "`P(wi|Spam)=Nwi|Spam + α / NSpam + α ⋅ NVocabulary`\n",
    "\n",
    "`P(wi|Ham)=Nwi|Ham + α / NHam + α ⋅ NVocabulary`\n",
    "\n",
    "Now some of the terms in the four equations above will have the same value for every new message. As a start, let's first calculate:\n",
    "\n",
    "* P(Spam) and P(Ham)\n",
    "* NSpam, NHam, NVocabulary\n",
    "\n",
    "    * NSpam is equal to the number of words in all the spam messages — it's not equal to the number of spam messages, and it's not equal to the total number of unique words in spam messages.\n",
    "    * NHam is equal to the number of words in all the non-spam messages — it's not equal to the number of non-spam messages, and it's not equal to the total number of unique words in non-spam messages.\n",
    "    * We'll also use Laplace smoothing and set α=1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the training set only:\n",
    "\n",
    "#Calculating P(Spam) and P(Ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "#Isolating spam and ham messages data first\n",
    "spam_messages = final_training_set[final_training_set['Label'] == 'spam']\n",
    "ham_messages = final_training_set[final_training_set['Label'] == 'ham']\n",
    "\n",
    "#P(Spam) and P(Ham)\n",
    "p_spam = int(round((len(spam_messages) / len(final_training_set)) * 100, 0))\n",
    "p_ham = int(round((len(ham_messages) / len(final_training_set)) * 100, 0))\n",
    "print(p_spam)\n",
    "print(p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15190\n",
      "57237\n",
      "7783\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Calculate NSpam, NHam, NVocabulary\n",
    "#N_Spam\n",
    "n_words_per_spam_message = spam_messages['SMS'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "print(n_spam)\n",
    "\n",
    "#N_Ham\n",
    "n_words_per_ham_message = ham_messages['SMS'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "print(n_ham)\n",
    "\n",
    "#N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "print(n_vocabulary)\n",
    "\n",
    "#Initializing a variable named alpha with a value of 1\n",
    "#Laplace smoothing\n",
    "alpha = 1\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned earlier, all the above terms will have constant values in our equations for every new message (regardless of the message or each individual word in the message)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, P(wi|Spam) and P(wi|Ham) will vary depending on the individual words. Although both P(wi|Spam) and P(wi|Ham) vary depending on the word, but the probability for each individual word is constant for every new message.\n",
    "\n",
    "We will be using the training set to get the values we need to find a result for both the equations below:\n",
    "\n",
    "`P(wi|Spam) = Nwi|Spam + α / NSpam + α ⋅ NVocabulary`\n",
    "\n",
    "`P(wi|Ham) = Nwi|Ham + α / NHam + α ⋅ NVocabulary`\n",
    "\n",
    "The steps we take to calculate P(wi|Spam) or P(wi|Ham) will be identical for every other new messages that contains the word `wi`. The key detail here is that calculating P(wi|Spam) or P(wi|Ham) only depends on the training set, and as long as we don't make changes to the training set, P(wi|Spam) or P(wi|Ham) stays constant. This means that we can use our training set to calculate the probability for both Spam and Ham of each word in our vocabulary. \n",
    "\n",
    "We have 7,783 words in our vocabulary, which means we'll need to calculate a total of 15,566 probabilities, as for each word we need to calculate both P(wi|Spam) and P(wi|Ham).\n",
    "\n",
    "In more technical language, the probability values that P(wi|Spam) and P(wi|Ham) will take are called parameters.\n",
    "\n",
    "The fact that we calculate so many values even before beginning with the classification of new messages makes the Naive Bayes algorithm very fast (especially compared to other algorithms). When a new message comes in, most of the needed computations are already done, which enables the algorithm to almost instantly classify the new message.\n",
    "\n",
    "If we didn't calculate all these values beforehand, then all these calculations would need to be done every time a new message comes in. Imagine the algorithm to be used to classify 1,000,000 new messages. Why repeat all these calculations 1,000,000 times when we could just do them once at the beginning?\n",
    "\n",
    "Let's now calculate all the parameters using the equations below:\n",
    " \n",
    "`P(wi|Spam) = Nwi|Spam + α / NSpam + α ⋅ NVocabulary`\n",
    "\n",
    "`P(wi|Ham) = Nwi|Ham + α / NHam + α ⋅ NVocabulary`\n",
    "\n",
    "As P(wi|Spam) and P(wi|Ham) are key parts of the equations that we need to classify the new messages:\n",
    "\n",
    "`P(Spam|w1,w2,...,wn) ∝ P(Spam) ⋅ ∏i=1nP(wi|Spam)`\n",
    "\n",
    "`P(Ham|w1,w2,...,wn) ∝ P(Ham) ⋅ ∏i=1nP(wi|Ham)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing two dictionaries, one dictionary to store the parameters for P(wi|Spam) and the other for P(wi|Ham), \n",
    "#where each key-value pair is a unique word (from our vocabulary) represented as a string and the value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    \n",
    "    n_word_given_ham = ham_messages[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've calculated all the constants and parameters we require, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "* Takes in as input a new message (w1, w2, ..., wn)\n",
    "* Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "* Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "    * If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    * If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    * If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a little briefing how our spam filter function (classify() function) works:\n",
    "\n",
    "* The input variable message is assumed to be a string.\n",
    "* We perform a bit of data cleaning on the string message:\n",
    "    * We remove the punctuation using the re.sub() function.\n",
    "    * We bring all letters to lower case using the str.lower() method.\n",
    "    * We split the string at the space character and transform it into a Python list using the str.split() method.\n",
    "* Calculate p_spam_given_message and p_ham_given_message:\n",
    "\n",
    "    `P(Spam|w1,w2,...,wn) ∝ P(Spam) ⋅ ∏i=1nP(wi|Spam)`\n",
    "    \n",
    "    `P(Ham|w1,w2,...,wn) ∝ P(Ham) ⋅ ∏i=1nP(wi|Ham)`\n",
    "\n",
    "\n",
    "* We compare p_spam_given_message with p_ham_given_message and then print a classification label.\n",
    "* If some new messages will contain words that are not part of the vocabulary, then we will simply ignore these words when we're calculating the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "            \n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifying two new messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.3021578215095486e-23\n",
      "P(Ham|message): 1.947076294334492e-25\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 2.354127765568133e-23\n",
      "P(Ham|message): 3.7070863895712615e-19\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy\n",
    "\n",
    "The two outputs above look promising, but let's see how well the filter does on our test set, which has 1,114 messages.\n",
    "\n",
    "The algorithm will output a classification label for every message in our test set, which we'll be able to compare with the actual label (given by a human). Note that, in training, our algorithm didn't see these 1,114 messages, so every message in the test set is practically new from the perspective of the algorithm.\n",
    "\n",
    "First off, we'll change the classify() function that we wrote previously to return the labels instead of printing them. Below, note that we now have return statements instead of print() functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function that returns labels instead of printing them, we can use it to create a new column in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS predicted\n",
       "0   ham          Later i guess. I needa do mcat study too.       ham\n",
       "1   ham             But i haf enuff space got like 4 mb...       ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...       ham"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compare the predicted values with the actual values to measure how good our spam filter is with classifying new messages. To make the measurement, we'll use accuracy as a metric:\n",
    "\n",
    "Accuracy = number of correctly classified messages / total number of classified messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measuring the accuracy of our spam filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.74\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = len(test_set)\n",
    "    \n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']:\n",
    "        correct += 1\n",
    "accuracy= round((correct/total)*100, 2)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above output of 98.74%, our spam filter seems functionally really good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding\n",
    "\n",
    "In this project, we managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The filter had an accuracy of 98.74% on the test set, which is an excellent result. We initially aimed for an accuracy of over 80%, but we managed to do way better than that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
